{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate word2vec embeddings using TensorFlow\n",
    "\n",
    "Uses a Softmax layer for prediction, cross-entropy for loss\n",
    "\n",
    "Modified from original code here: https://www.tensorflow.org/tutorials/word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some imports to make code compatible with Python 2 as well as 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import zipfile\n",
    "import scipy\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from six.moves import urllib\n",
    "from six.moves import xrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sbamu/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.7\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DOWNLOADED_FILENAME = 'SampleText.zip'\n",
    "# check whether it has been downloaded before and retrieve it\n",
    "def maybe_download(url_path, expected_bytes):\n",
    "    if not os.path.exists(DOWNLOADED_FILENAME):\n",
    "        filename, _ = urllib.request.urlretrieve(url_path, DOWNLOADED_FILENAME)\n",
    "    statinfo = os.stat(DOWNLOADED_FILENAME)\n",
    "\n",
    "    if statinfo.st_size == expected_bytes: #check size of file\n",
    "        print('Found and verified file from this path: ', url_path)\n",
    "        print('Downloaded file: ', DOWNLOADED_FILENAME)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "\n",
    "        raise Exception(\n",
    "            'Failed to verify file from: ' + url_path + '. Can you get to it with a browser?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wish to extract the contents of this file and parse it into individual words. We use *tf.compat.as_str()* to to convert the contents into string format. The *.split()* extracts the words for the file stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_words():\n",
    "    with zipfile.ZipFile(DOWNLOADED_FILENAME) as f:\n",
    "        firstfile = f.namelist()[0] # read the first file\n",
    "        filestring = tf.compat.as_str(f.read(firstfile)) # convert it to string\n",
    "        words = filestring.split()\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified file from this path:  http://mattmahoney.net/dc/text8.zip\n",
      "Downloaded file:  SampleText.zip\n"
     ]
    }
   ],
   "source": [
    "URL_PATH = 'http://mattmahoney.net/dc/text8.zip'\n",
    "FILESIZE = 31344016\n",
    "\n",
    "maybe_download(URL_PATH, FILESIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the list of words from the the sample, this is our entire vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabulary = read_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17005207"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anarchism',\n",
       " 'originated',\n",
       " 'as',\n",
       " 'a',\n",
       " 'term',\n",
       " 'of',\n",
       " 'abuse',\n",
       " 'first',\n",
       " 'used',\n",
       " 'against',\n",
       " 'early',\n",
       " 'working',\n",
       " 'class',\n",
       " 'radicals',\n",
       " 'including',\n",
       " 'the',\n",
       " 'diggers',\n",
       " 'of',\n",
       " 'the',\n",
       " 'english',\n",
       " 'revolution',\n",
       " 'and',\n",
       " 'the',\n",
       " 'sans',\n",
       " 'culottes',\n",
       " 'of']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary[:26]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the data set used to generate word2vec embeddings\n",
    "\n",
    "* *words* A list of all words in the input dataset\n",
    "* *n_words* The number of words to include in the dataset. We use the most frequently occurring n_words\n",
    "\n",
    "Return values are:\n",
    "\n",
    "* *word_counts* - array of arrays - The most frequently occurring words and the corresponding frequencies\n",
    "> we only intend to create word embeddings for the 10 most frequent words - therefore all the rest are added to 'UNKNOWN'\n",
    "* *word_indexes* The list of index values which uniquely identifies each word in the dataset - higher frequency words will have a lower index values.\n",
    "* *dictionary* Mapping from a word to its unique index\n",
    "* *reversed_dictionary* Mapping from the unique index to the word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_dataset(words, n_words):\n",
    "    word_counts = [['UNKNOWN', -1]]\n",
    "    \n",
    "    counter = collections.Counter(words) #allows access to frequently used words\n",
    "    word_counts.extend(counter.most_common(n_words - 1)) \n",
    "    #adds the most common words to the word_counts array\n",
    " \n",
    "    # Only the most common words are added to the dictionary\n",
    "    dictionary = dict()\n",
    "    \n",
    "    for word, _ in word_counts:\n",
    "        # The current length of the dictionary is the unique index of this word\n",
    "        # added to the dictionary\n",
    "        dictionary[word] = len(dictionary)\n",
    "    \n",
    "    word_indexes = list()\n",
    "    \n",
    "    unknown_count = 0\n",
    "    for word in words:\n",
    "        if word in dictionary: #if it is in the top n_words\n",
    "            index = dictionary[word]\n",
    "        else:\n",
    "            index = 0  # refers to dictionary['UNKNOWN']\n",
    "            unknown_count += 1\n",
    "        \n",
    "        word_indexes.append(index) \n",
    "        #appends the index of the current word to the word index list\n",
    "    \n",
    "    # Count of unknown words\n",
    "    word_counts[0][1] = unknown_count\n",
    "    \n",
    "    # Map from unique indexes to word\n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    \n",
    "    return  word_counts, word_indexes, dictionary, reversed_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VOCABULARY_SIZE = 5000\n",
    "\n",
    "word_counts, word_indexes, dictionary, reversed_dictionary = build_dataset(vocabulary, VOCABULARY_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['UNKNOWN', 2735459], ('the', 1061396), ('of', 593677), ('and', 416629), ('one', 411764), ('in', 372201), ('a', 325873), ('to', 316376), ('zero', 264975), ('nine', 250430)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(word_counts[:10])\n",
    "len(word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 3081, 12, 6, 195, 2, 3134, 46, 59, 156]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "17005207"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(word_indexes[:10])\n",
    "len(word_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dependent : 2746\n",
      "theatre : 2193\n",
      "changes : 721\n",
      "musical : 859\n",
      "game : 183\n",
      "foot : 1971\n",
      "via : 1166\n",
      "consequence : 3767\n",
      "aka : 4061\n",
      "volcanic : 4788\n",
      "UNKNOWN: 0\n",
      "5000\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "for key in random.sample(list(dictionary), 10):\n",
    "    print(key, \":\", dictionary[key])\n",
    "    \n",
    "print(\"UNKNOWN:\", dictionary[\"UNKNOWN\"])\n",
    "print(len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1033 : recorded\n",
      "1973 : count\n",
      "4520 : jump\n",
      "2380 : agent\n",
      "84 : war\n",
      "4992 : statute\n",
      "3743 : counties\n",
      "3229 : compact\n",
      "1674 : twenty\n",
      "4758 : registered\n",
      "0: UNKNOWN\n"
     ]
    }
   ],
   "source": [
    "for key in random.sample(list(reversed_dictionary), 10):\n",
    "    print(key, \":\", reversed_dictionary[key])\n",
    "    \n",
    "print(\"0:\",reversed_dictionary[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Return one batch of data and the corresponding labels for training\n",
    "\n",
    "* *word_indexes* A list of unique indexes which identifies each word in the dataset (17005207 words). The most popular words have the lowest index values\n",
    "* *batch_size* The number of elements in each batch\n",
    "* *num_skips* The number of words to choose at random from the neighboring words within the skip_window. Each input word will appear num_skips times in the batch with a context or neighboring word as the corresponding label\n",
    "* *skip_window* How many words to consider around one input word, also referred to as the context_window\n",
    "\n",
    "### *generate_batch()*\n",
    "* batch = [1, 2, 3, .... *batch_size*]\n",
    "* context = [[1], [2], [3], ..., [*batch_size*]]\n",
    "* span - The span of a window includes the *skip_window* elements on each side of the input word plus the word itself\n",
    "* deque - A deque is double-ended queue which supports memory efficient appends and pops from each side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Global index into words maintained across batches\n",
    "global_index = 0 # index into the list of words\n",
    "\n",
    "def generate_batch(word_indexes, batch_size, num_skips, skip_window):\n",
    "    global global_index\n",
    "\n",
    "    # For every input we find num_skips context words within a window\n",
    "    assert batch_size % num_skips == 0 # batch_size needs to be a multiple of num_skips\n",
    "    assert num_skips <= 2 * skip_window\n",
    "    \n",
    "    \n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    \n",
    "    span = 2 * skip_window + 1  # [ skip_window input_word skip_window ]\n",
    "\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "\n",
    "    # Initialize the deque with the first words in the deque\n",
    "    for _ in range(span):\n",
    "        buffer.append(word_indexes[global_index])\n",
    "        global_index = (global_index + 1) % len(word_indexes)\n",
    "        \n",
    "    # Each input word is used to predict num_skips target words \n",
    "    for i in range(batch_size // num_skips):\n",
    "        target = skip_window  # input word at the center of the buffer\n",
    "        targets_to_avoid = [skip_window] # the whole context window\n",
    "\n",
    "        for j in range(num_skips):\n",
    "            while target in targets_to_avoid:\n",
    "                target = random.randint(0, span - 1)\n",
    "            targets_to_avoid.append(target)\n",
    "            \n",
    "            batch[i * num_skips + j] = buffer[skip_window]  # this is the input word\n",
    "            labels[i * num_skips + j, 0] = buffer[target]  # these are the context words\n",
    "        \n",
    "        # The first word from the buffer is removed automatically when a new word\n",
    "        # is added in at the end\n",
    "        buffer.append(word_indexes[global_index])\n",
    "        global_index = (global_index + 1) % len(word_indexes)\n",
    "    \n",
    "    # Backtrack a little bit to avoid skipping words in the end of a batch, these\n",
    "    # words will be captured in the next batch\n",
    "    global_index = (global_index + len(word_indexes) - span) % len(word_indexes)\n",
    "\n",
    "    return batch, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch, labels = generate_batch(word_indexes, 10, 2, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   2,    2, 3134, 3134,   46,   46,   59,   59,  156,  156],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  59],\n",
       "       [3081],\n",
       "       [   2],\n",
       "       [  59],\n",
       "       [  59],\n",
       "       [ 742],\n",
       "       [ 156],\n",
       "       [ 742],\n",
       "       [ 477],\n",
       "       [  59]], dtype=int32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "of :  used\n",
      "of :  originated\n",
      "abuse :  of\n",
      "abuse :  used\n",
      "first :  used\n",
      "first :  working\n",
      "used :  against\n",
      "used :  working\n",
      "against :  class\n"
     ]
    }
   ],
   "source": [
    "for i in range(9):\n",
    "    print(reversed_dictionary[batch[i]], \": \", reversed_dictionary[labels[i][0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize some variables to build and train the skip-gram model\n",
    "\n",
    "* *batch_size*: The size of the batch to use in training **Hyperparameter**\n",
    "* *embedding_size*: Dimensions of the embedding vector i.e. how many features we use to represent a word - ie. the hidden layer has 300 neurons\n",
    "* *skip_window*: How many words to include in the context, this is to the left and right\n",
    "* *num_skips*: How many context words to associate with each target word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "embedding_size = 300\n",
    "skip_window = 2\n",
    "num_skips = 2\n",
    "\n",
    "# Reset the global index because we updated while testing the batch code\n",
    "global_index = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose some words at random to validate our trained model\n",
    "\n",
    "* *valid_size*: Number of words to evaluate our model, we'll use these words to see how similar the closest words are\n",
    "* *valid_window*: The window from where we draw the words to run validation on, only pick from the most commonly used words\n",
    "\n",
    "Choose the set of words from the top 100 at random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid_size = 16  \n",
    "valid_window = 100\n",
    "\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input placeholder to feed data in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128,)\n",
      "(128, 1)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "print(train_inputs.shape)\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up a constant to hold validation data\n",
    "The constant of random instances that we use to validate our training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid_dataset = tf.constant(valid_examples, dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings are word representations that will be generated by word2vec\n",
    "\n",
    "Initialize a variable to hold the embeddings and embeddings for specific words can be accessed using *tf.nn.embedding_lookup*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/sbamu/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "embeddings = tf.Variable(\n",
    "    tf.random_uniform([VOCABULARY_SIZE, embedding_size], -1.0, 1.0))\n",
    "embed = tf.nn.embedding_lookup(embeddings, train_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding matrix \n",
    "* embeddings [5000 x 300] - 5000 is the vocabulary size and 300 is the dimensionality of each embedding\n",
    "* embed tensor [128 x 300] - 128 is the batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(5000, 300) dtype=float32_ref>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'embedding_lookup/Identity:0' shape=(128, 300) dtype=float32>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed # unique indexes of each word in the batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct weights and biases for the Softmax prediction layer\n",
    "\n",
    "$y = Wx + b$\n",
    "These are the parameters we wish to determine during training.\n",
    "W = weights matrix [5000, 300]\n",
    "b = biases vector\n",
    "\n",
    "We also set up the hidden layer with no activation function - ie. linear layer $xW^T + b$ \n",
    "\n",
    "$xW^T$ = [128, 300].[300, 5000] = [128, 5000]\n",
    "\n",
    "$xW^T + b$ = [128, 5000] + [128, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Construct the variables for the softmax\n",
    "weights = tf.Variable(tf.truncated_normal([VOCABULARY_SIZE, embedding_size],\n",
    "                          stddev=1.0 / math.sqrt(embedding_size)))\n",
    "biases = tf.Variable(tf.zeros([VOCABULARY_SIZE]))\n",
    "\n",
    "hidden_out = tf.matmul(embed, tf.transpose(weights)) + biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the cross-entropy (loss function for Softmax prediction)\n",
    "\n",
    "Represent the labels in on-hot notation in order to input it into the Softmax Prediction layer for the loss calculation\n",
    "\n",
    "**The Softmax Prediction Layer**\n",
    "The Softmax layer is typically used for True/False Classification\n",
    "It uses the cross entropy (distance between probability distributions) for its loss function. Using logistic regression to compute the probability for the True case or False case.\n",
    "\n",
    "In Word Classification, the Softmax Prediction Layer computes a probability for each word in the Vocabulary - the word with the highest probability is the predicted target.\n",
    "\n",
    "By using gradient descent, we wish to determine the global/local minima in the loss function (ie. minimise the prediction error)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-52-7a0af00501e1>:3: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_one_hot = tf.one_hot(train_labels, VOCABULARY_SIZE)\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hidden_out, \n",
    "    labels=train_one_hot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/sbamu/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    " optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize the embeddings vector to calculate cosine similarity between words\n",
    "We wish to set up a metric to determine the similarity between the 2 words - Cosine Similarity.\n",
    "**Cosine Similarity** is a measure of distance between word embeddings which is used to check the similarity between the words.\n",
    "\n",
    "*normalized_vector = vector / L2 norm of vector*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l2_norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keepdims=True))\n",
    "normalized_embeddings = embeddings / l2_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look up the normalized embeddings of the words we use to validate our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'embedding_lookup_1/Identity:0' shape=(16, 300) dtype=float32>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_embeddings # 16 words in out validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'truediv_1:0' shape=(5000, 300) dtype=float32>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the cosine similarity of the validation words against all words in our vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_steps = 20001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  0 :  8.622496604919434\n",
      "Nearest to world: classified, thousands, sam, column, victory, strategy, opposed, why,\n",
      "Nearest to seven: voted, learned, voices, circuit, edgar, do, poem, society,\n",
      "Nearest to have: greek, studies, planning, band, unless, memorial, smallest, silver,\n",
      "Nearest to nine: showing, enemies, mineral, agriculture, belgium, entity, controls, organisations,\n",
      "Nearest to of: pictures, transformed, lords, haiti, ipa, northeast, suit, ball,\n",
      "Nearest to war: pressure, bed, castle, franklin, fans, wearing, suggests, standard,\n",
      "Nearest to UNKNOWN: complexity, ft, broadway, speaker, graduate, developed, thin, arts,\n",
      "Nearest to for: expected, several, uniform, time, primary, quantity, became, climate,\n",
      "Nearest to his: genes, perception, classes, la, everyone, negotiations, adaptation, conduct,\n",
      "Nearest to however: stayed, totally, starting, republican, game, bought, cell, table,\n",
      "Nearest to not: survival, photo, margaret, duncan, fired, interest, unlikely, familiar,\n",
      "Nearest to only: dates, relations, drum, statements, quantity, adolf, audience, assumption,\n",
      "Nearest to while: intervention, particular, norwegian, verb, method, involves, readers, passes,\n",
      "Nearest to so: unofficial, played, observer, reform, horses, geographic, upon, minutes,\n",
      "Nearest to time: thrown, study, for, historically, fine, days, judicial, genre,\n",
      "Nearest to is: dimension, most, vowels, category, drive, stack, climate, depth,\n",
      "\n",
      "\n",
      "Average loss at step  2000 :  6.009429234743118\n",
      "Average loss at step  4000 :  5.6721978943347935\n",
      "Average loss at step  6000 :  5.678996883273125\n",
      "Average loss at step  8000 :  5.609019051015377\n",
      "Average loss at step  10000 :  5.4347520418167115\n",
      "Nearest to world: classified, thousands, sam, strategy, opposed, why, victory, raised,\n",
      "Nearest to seven: isbn, learned, circuit, voted, symbolic, voices, cable, edgar,\n",
      "Nearest to have: planning, smallest, greek, element, draw, silver, unless, urban,\n",
      "Nearest to nine: zero, showing, agriculture, mineral, five, eight, signs, entity,\n",
      "Nearest to of: pictures, transformed, powered, ipa, via, in, turkish, lords,\n",
      "Nearest to war: pressure, fans, suggests, existing, abandoned, franklin, standard, castle,\n",
      "Nearest to UNKNOWN: opens, who, he, term, coal, responsibility, desert, a,\n",
      "Nearest to for: into, with, expected, time, became, uniform, several, climate,\n",
      "Nearest to his: genes, perception, la, classes, negotiations, everyone, the, r,\n",
      "Nearest to however: totally, stayed, republican, game, bought, starting, cell, table,\n",
      "Nearest to not: survival, margaret, duncan, photo, fired, unlikely, interest, refers,\n",
      "Nearest to only: dates, relations, drum, statements, children, audience, universities, adolf,\n",
      "Nearest to while: intervention, particular, norwegian, method, verb, passes, engineer, readers,\n",
      "Nearest to so: unofficial, played, reform, geographic, observer, upon, spain, opportunity,\n",
      "Nearest to time: study, for, historically, thrown, fine, judicial, replacement, days,\n",
      "Nearest to is: most, vowels, drive, dimension, man, continent, carry, trust,\n",
      "\n",
      "\n",
      "Average loss at step  12000 :  5.508625409960747\n",
      "Average loss at step  14000 :  5.374250762701035\n",
      "Average loss at step  16000 :  5.250667449474335\n",
      "Average loss at step  18000 :  5.217448168963194\n",
      "Average loss at step  20000 :  5.499518897533417\n",
      "Nearest to world: classified, thousands, sam, strategy, opposed, why, victory, raised,\n",
      "Nearest to seven: zero, nine, isbn, learned, symbolic, voted, voices, circuit,\n",
      "Nearest to have: planning, smallest, greek, silver, element, unless, memorial, draw,\n",
      "Nearest to nine: zero, one, eight, two, five, six, seven, three,\n",
      "Nearest to of: in, for, pictures, big, and, via, ipa, transformed,\n",
      "Nearest to war: pressure, abandoned, fans, suggests, castle, franklin, existing, standard,\n",
      "Nearest to UNKNOWN: a, famous, the, wheel, hard, democracy, country, honor,\n",
      "Nearest to for: of, with, climate, expected, into, became, several, brass,\n",
      "Nearest to his: genes, perception, the, la, classes, everyone, negotiations, adaptation,\n",
      "Nearest to however: totally, stayed, game, republican, bought, starting, cell, table,\n",
      "Nearest to not: survival, refers, unlikely, margaret, produce, duncan, interest, photo,\n",
      "Nearest to only: dates, drum, relations, statements, children, adolf, audience, universities,\n",
      "Nearest to while: intervention, norwegian, particular, involves, engineer, verb, readers, passes,\n",
      "Nearest to so: unofficial, reform, geographic, observer, played, upon, brothers, opportunity,\n",
      "Nearest to time: study, historically, thrown, fine, replacement, judicial, for, invention,\n",
      "Nearest to is: was, vowels, drive, carry, dimension, stack, continent, man,\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    init.run()\n",
    "\n",
    "    average_loss = 0\n",
    "    for step in xrange(num_steps):\n",
    "        batch_inputs, batch_labels = generate_batch(\n",
    "            word_indexes, batch_size, num_skips, skip_window)\n",
    "        feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}\n",
    "\n",
    "        # We perform one update step by evaluating the optimizer op (including it\n",
    "        # in the list of returned values for session.run()\n",
    "        _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "        average_loss += loss_val\n",
    "\n",
    "        if step % 2000 == 0:\n",
    "            if step > 0:\n",
    "                average_loss /= 2000\n",
    "            \n",
    "            # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "            print('Average loss at step ', step, ': ', average_loss)\n",
    "            average_loss = 0\n",
    "\n",
    "        # Note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "        if step % 10000 == 0:\n",
    "            sim = similarity.eval()\n",
    "            for i in xrange(valid_size):\n",
    "                valid_word = reversed_dictionary[valid_examples[i]]\n",
    "                top_k = 8  # number of nearest neighbors\n",
    "                nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
    "                log_str = 'Nearest to %s:' % valid_word\n",
    "                for k in xrange(top_k):\n",
    "                    close_word = reversed_dictionary[nearest[k]]\n",
    "                    log_str = '%s %s,' % (log_str, close_word)\n",
    "                print(log_str)\n",
    "            print(\"\\n\")    \n",
    "    final_embeddings = normalized_embeddings.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
